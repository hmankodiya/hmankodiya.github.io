<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html lang="en">

<head>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */

    a {
      color: #1772d0;
      text-decoration: none;
    }

    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }


    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
    }

    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
    }

    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 22px;
    }

    roletitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 18px;
      font-weight: 700
    }

    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
      font-weight: 700
    }

    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 32px;
    }

    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }

    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    span.highlight {
      background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/jpg" href="images/profile_old.png">
  <title>Harsh Mankodiya</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet'
    type='text/css'>
</head>

<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>

            <td width="67%" valign="middle">
              <p align="center">
                <name>Harsh Mankodiya</name>
              </p>
              <p>Hi! I am Harsh Mankodiya, a second-year Masterâ€™s student in Computer Science at Arizona State
                University, specializing in Machine Learning and Artificial Intelligence. I am passionate about building
                scalable AI systems that bridge research and real-world applications. My work spans Multimodal Learning,
                Reinforcement Learning (RL), and Natural Language Processing (NLP), with a strong focus on developing
                adaptable AI solutions across diverse domains.

                I specialize in translating theoretical machine learning concepts into scalable systems, optimizing
                model pipelines for performance, and designing frameworks tailored to specific applications.

              </p>
              <p>Previously, I received a B.Tech. in Computer Science from Nirma University.
              </p>
              <p>I am originally from Gujarat, India, and outside of research, I enjoy spending time taking long walks,
                playing video games, and socializing.
              </p>
              <p style="background-color: yellow; font-weight: bold; padding: 5px; display: inline-block;">
                Seeking full-time Machine Learning or Data Scientist opportunities for 2025.
              </p>

              </p>
              <p align="center">[<a href="mailto:hmankodi@asu.edu">Email</a>]
                [<a href="data/Harsh_Mankodiya_resume.pdf">CV</a>]
                [<a href="https://scholar.google.com/citations?user=XM9XxxgAAAAJ&hl=en">Google Scholar</a>]
                [<a href="https://www.linkedin.com/in/harshmankodiya/">LinkedIn</a>]
                [<a href="https://github.com/hmankodiya">GitHub</a>]
                <!-- [<a href="BLOG_PLACEHOLDER" style="color: red;">BLOG</a>] -->
              </p>
            </td>

            <td width="10%">
              <img src="images/profile.jpg" width="220" height="240">
            </td>

          </tr>
        </table>
        <hr style="margin: 10px;">
        <!-- ################## WORK EXP ################## -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">

          <tr>
            <td width="100%" valign="middle">
              <heading>Professional Experience</heading>
            </td>

          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <!-- cellpadding="10" style="padding: 0px;"> -->

          <tr>
            <td valign="middle" width="75%">
              <!-- style="padding-left: 10px; padding-bottom: 10px; padding-top: 10px; padding-right: 10px;"> -->
              <roletitle>Machine Learning Intern</roletitle>
              <br>
              <em style="font-size: 15px;">Cellino Biotech, Cambridge, MA</em>
              <br>
              <em style="font-size: 12px;">May 2024 - August 2024</em>
              <br>
              <br>
              Developed a PoC for a central embedding model leveraging pretrained architectures for downstreaming patch
              selection,
              anomaly detection, and segmentation, achieving an 82% F1-Score by fine-tuning DinoV2 with ViT-based heads.
            </td>
          </tr>

          <tr>
            <td valign="middle" width="75%">
              <!-- style="padding-left: 10px; padding-bottom: 10px; padding-top: 10px; padding-right: 10px;"> -->
              <roletitle>Graduate Researcher</roletitle>
              <br>
              <em style="font-size: 15px;">LENS Lab, ASU, Tempe, AZ</em>
              <br>
              <em style="font-size: 12px;">August 2023 - May 2024</em>
              <br>
              <br>
              Integrated eXplainable AI with autonomous vehicle agents in simulation environments like Carla and
              Gymnasium.
              Trained PPO with VAE-based feature extraction using StableBaselines3 and employed CLIP models for
              zero-shot
              segmentation and concept sampling.
            </td>
          </tr>

          <tr>
            <td valign="middle" width="75%">
              <!-- style="padding-left: 10px; padding-bottom: 10px; padding-top: 10px; padding-right: 10px;"> -->
              <roletitle>Research Intern</roletitle>
              <br>
              <em style="font-size: 15px;">Bosch (AIShield), Bengaluru, India</em>
              <br>
              <em style="font-size: 12px;">Jan 2023 - May 2024</em>
              <br>
              <br>
              Formulated a novel Knowledge Distillation methodology using GradCAM for image
              segmentation models. Leveraged PyTorch Lightning to streamline data processing, model training,
              evaluation, and inference, with experiment tracking via MLFlow. Trained SegNet and U-Net segmentation
              models on NVIDIA DGX A100 systems, achieving high relative IoU scores exceeding 85% across multiple
              datasets.
            </td>
          </tr>

          <tr>
            <td valign="middle" width="75%">
              <!-- style="padding-left: 10px; padding-bottom: 10px; padding-top: 10px; padding-right: 10px;"> -->
              <roletitle>Machine Learning Intern</roletitle>
              <br>
              <em style="font-size: 15px;">Samyak Infotech, Ahmedabad, India</em>
              <br>
              <em style="font-size: 12px;">June 2022 - July 2022</em>
              <br>
              <br>
              Trained a BERT-based LMLayout model for business invoice information extraction, achieving an F1-Score of
              81%.
              Designed labeling criteria, annotated 200+ invoices, integrated labels into training pipelines, and
              managed a
              surrogate SQL database for efficient data retrieval.
            </td>
          </tr>

          <tr>
            <td valign="middle" width="75%">
              <!-- style="padding-left: 10px; padding-bottom: 10px; padding-top: 10px; padding-right: 10px;"> -->
              <roletitle>Undergraduate Researcher</roletitle>
              <br>
              <em style="font-size: 15px;">STLabs, Nirma University, Ahmedabad, India</em>
              <br>
              <em style="font-size: 12px;">August 2021 - May 2023</em>
              <br>
              <br>
              Collaborated with researchers, Ph.D. students, and undergraduates on projects in Computer Vision, Deep
              Learning,
              and Explainable AI (XAI).
            </td>
          </tr>

        </table>
        <!-- ################## WORK EXP END ################## -->

        <hr style="margin: 10px;">

        <!-- ################## RESEARCH ################## -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">

          <tr>
            <td width="100%" valign="middle">
              <heading>Selected Research</heading>
              <p> Please see my <a href="https://scholar.google.com/citations?user=XM9XxxgAAAAJ&hl=en">Google
                  Scholar</a>
                for
                a full list of work.
              </p>
            </td>

          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">

          <tr>
            <td width="25%">
              <div class="one">
                <div class="two"><img src='images/Neurips_SATA.png' width="160" height="140" vspace="20"></div>
              </div>
            </td>
            <td valign="middle" width="75%">
              <a href="https://arxiv.org/abs/2409.10733">
                <papertitle>Trustworthy Conceptual Explanations for Neural Networks in Robot Decision-Making
                </papertitle>
              </a>
              <br>
              Som Sagar*, Aditya Taparia*, <strong>Harsh Mankodiya</strong>, Pranav Bidare, Yifan Zhou, Ransalu
              Senanayake
              <br>
              <em>NeurIPS Workshop on Safe & Trustworthy Agents</em>, 2024
              <br>
              [<a href="https://arxiv.org/pdf/2409.10733.pdf">PDF</a>]
              <br>
              We introduce BaTCAV, a Bayesian TCAV framework with uncertainty estimations that enhances the
              interpretability of robotic actions across both simulation platforms and real-world robotic systems.
            </td>
          </tr>

          <tr>
            <td width="25%">
              <div class="one">
                <div class="two"><img src='images/od-xai.webp' width="160" height="140" vspace="20"></div>
              </div>
            </td>
            <td valign="middle" width="75%">
              <a href="https://www.mdpi.com/2076-3417/12/11/5310">
                <papertitle>OD-XAI: Explainable AI-Based Semantic Object Detection for Autonomous Vehicles
                </papertitle>
              </a>
              <br>
              <strong>Harsh Mankodiya</strong>, Dhairya Jadav, Rajesh Gupta, Sudeep Tanwar
              <br>
              <em>MDPI Applied Sciences</em>, 2024
              <br>
              [<a href="https://www.mdpi.com/2076-3417/12/11/5310">PDF</a>]
              <br>
              We propose an XAI integrated AV system that improves the explainability of semantic segmentation models,
              which
              are considered black box models and are difficult to analyze and comprehend.
            </td>
          </tr>

        </table>
        <!-- ################## RESEARCH END ################## -->

        <hr style="margin: 10px;">

        <!-- ################## PROJECTS ################## -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">

          <tr>
            <td width="100%" valign="middle">
              <heading>Projects</heading>
            </td>

          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">

          <tr>
            <td width="25%">
              <div class="one">
                <div class="two"><img src='images/lora_diagram.png' width="150" height="140" vspace="20"></div>
              </div>
            </td>
            <td valign="middle" width="75%">
              <a href="https://github.com/hmankodiya/LlamaSentimentClassification">
                <papertitle>Multilingual Sentiment Classification using LLMs
                </papertitle>
              </a>
              <br>
              [<a href="https://github.com/hmankodiya/LlamaSentimentClassification">Code</a>]
              <br>
              Fine-tuned LLaMA2-7B using Quantized Low-Rank Adaptation (Q-LoRA) for multilingual sentiment analysis
              across 12 languages, achieving a 30% increase in test AUC and a 20% improvement in accuracy. Trained on
              datasets like IndoNLU, GoEmotions, and multilingual Amazon reviews, encompassing diverse domains such as
              social media, e-commerce, and movie reviews. Conducted comparative analysis with GPT-2 and BERT,
              showcasing LLaMA2-7B's superior performance with minimal trainable parameters.
            </td>
          </tr>

          <tr>
            <td width="25%">
              <div class="one">
                <div class="two"><img src='images/ShowAndTell.png' width="150" height="140" vspace="20"></div>
              </div>
            </td>
            <td valign="middle" width="75%">
              <a href="https://github.com/hmankodiya/PaperImplementations/tree/master/ShowAndTell">
                <papertitle>Transformer-Powered Image Captioning with pre-trained DinoV2 embeddings
                </papertitle>
              </a>
              <br>
              [<a href="https://github.com/hmankodiya/PaperImplementations/tree/master/ShowAndTell">Code</a>]
              <br>
              Developed a caption generation model leveraging the CLIP Vision encoder and DINOv2 transformer embeddings
              trained on the MS COCO Captions dataset. Integrated and fine-tuned GPT-2 decoder, on ~1% of the MS COCO
              dataset, which
              achieved a BLEU-4 score of 7%. Utilized a pre-trained GPT-2 tokenizer for efficient caption tokenization.
              Implemented and evaluated diverse decoding strategies, including greedy decoding and beam search, to
              optimize caption generation. The model demonstrated robust performance, showcasing its capability for
              high-quality image-to-text generation.
          </tr>

        </table>
        <!-- ################## PROJECTS END ################## -->

        <hr style="margin: 10px;">
      </td>
    </tr>

  </table>

  <!-- WEBSITE TEMPLATE-->
  <table width="100%" cellspacing="0" cellpadding="20" border="0" align="center">
    <tbody>
      <tr>
        <td>
          <br>
          <p align="center">
            <font size="2">
              Website template from <a href="https://github.com/jonbarron/jonbarron_website">here.</a>
            </font>
          </p>
        </td>
      </tr>
    </tbody>
  </table>
  <!-- WEBSITE TEMPLATE END-->

</body>

</html>